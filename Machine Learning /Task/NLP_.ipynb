{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMJGiFYlGEEa",
        "outputId": "7d0da692-eb2f-4e9a-9b42-ab5b4f181db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.18.0.post0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiAFt8olEuGY",
        "outputId": "9dbc70b4-ccc0-4079-83d9-534f4e753698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample paragraph\n",
        "text = \"\"\"\n",
        "Machine learning (ML) is a branch of artificial intelligence (AI) and computer\n",
        "science that focuses on the using data and algorithms\n",
        "to enable AI to imitate the way that humans learn, gradually improving its accuracy.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "7Gs7eL3vE2J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq-Vkh1LFaNS",
        "outputId": "062f21a6-91f5-4ee0-c55f-cf3b1fb5036e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Machine', 'learning', '(', 'ML', ')', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'and', 'computer', 'science', 'that', 'focuses', 'on', 'the', 'using', 'data', 'and', 'algorithms', 'to', 'enable', 'AI', 'to', 'imitate', 'the', 'way', 'that', 'humans', 'learn', ',', 'gradually', 'improving', 'its', 'accuracy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlLANo0JFlFc",
        "outputId": "24cc272a-cc85-49e9-82aa-805e212189e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['Machine', 'learning', '(', 'ML', ')', 'branch', 'artificial', 'intelligence', '(', 'AI', ')', 'computer', 'science', 'focuses', 'using', 'data', 'algorithms', 'enable', 'AI', 'imitate', 'way', 'humans', 'learn', ',', 'gradually', 'improving', 'accuracy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPRS7_4IFuTA",
        "outputId": "860c7ece-aa70-470f-944a-05cd13967938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['Machine', 'learning', '(', 'ML', ')', 'branch', 'artificial', 'intelligence', '(', 'AI', ')', 'computer', 'science', 'focus', 'using', 'data', 'algorithm', 'enable', 'AI', 'imitate', 'way', 'human', 'learn', ',', 'gradually', 'improving', 'accuracy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Get the sentiment\n",
        "sentiment = blob.sentiment\n",
        "print(\"\\nSentiment Analysis:\")\n",
        "print(f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXohJES7FzyR",
        "outputId": "5564925a-665e-4883-c624-ff2fd04d7892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentiment Analysis:\n",
            "Polarity: -0.6, Subjectivity: 1.0\n"
          ]
        }
      ]
    }
  ]
}